---
data_args:
  # Data training arguments
  dataset: wikivitals-lvl5-04-2022 # the name of the dataset (used only to display infos)
  max_seq_length: 128 # maximum number of tokens
  overwrite_cache: True 
  pad_to_max_length: True # pad to maximum length if True else no padding
  path_base: './datasets/wikivitals-lvl5-04-2022/' # the path to the dataset
  train_file: wikivitals-lvl5-04-2022-dense_train.json
  validation_file: wikivitals-lvl5-04-2022-dense_val.json
  test_file: wikivitals-lvl5-04-2022-dense_test.json
  fill_label_value_strategy: ~ # the strategy to adopt if some labels are missing
  task_labels_col: 'label'  # the column (for CSV files) of field (for JSON files) for the labels
  task_inputs_col: 'abstract' # the column (for CSV files) of field (for JSON files) for the input text
  multilabel_classification: False # single label classification
  other_args: 
    depth: 1 # 1 (coarse), 2 (intermediary) or 3 (fine) for wikivitals-lvl5-04-2022
  debug_mode: False # activates debug mode if True (selection of a subset of the dataset for debugging purpose)
  debug_split_sizes: 1000 # used only if debug_mode is True, size of the subsets of train, validation, and test sets

model_args:
  # Model arguments
  name_or_path: 'bert-base-uncased' # bert model to use (one can also specify a path)  ex: 'bert-base-uncased'
  classification_head: 'base_sequence_classifier' # value among 'base_sequence_classifier', 'with_word_attention'
  hidden_dropout_prob: 0.2 # dropout probability for the classification head

preliminary_training_args: 
  do_train: False

training_args:
  # Trainer arguments
  do_train: True
  do_eval: False
  do_predict: False
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  optim: 'adamw_torch'
  gradient_accumulation_steps: 1
  eval_accumulation_steps: 1
  learning_rate: 3.e-5 # don't forget the '.'
  weight_decay: 0.1
  num_train_epochs: 2
  lr_scheduler_type: constant_with_warmup # ['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup', 'inverse_sqrt', 'reduce_lr_on_plateau']
  warmup_steps: 1000
  logging_steps: 100
  seed: 42
  label_names: ~ # defaults to 'labels'
  report_to: 'tensorboard'
  save_total_limit: 1 # limits the total amount of checkpoints. Deletes the older checkpoints in output_dir.
  load_best_model_at_end: True
  metric_for_best_model: 'eval_loss'
  save_strategy: 'steps' # in ['no', 'steps', 'epoch'], don't forget the quotes if 'no'
  evaluation_strategy: 'steps'
  save_steps: 200
  eval_steps: 200
  output_dir: ./outputs/models 
