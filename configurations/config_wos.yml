---
data_args:
  # Data training arguments
  dataset: wos # the name of the dataset (used only to display infos)
  max_seq_length: 512 # maximum number of tokens
  overwrite_cache: True 
  pad_to_max_length: True # pad to maximum length if True else no padding
  path_base: './datasets/WOS/' # the path to the dataset
  train_file: wos_train_raw.json
  validation_file: wos_val_raw.json 
  test_file: wos_test_raw.json
  fill_label_value_strategy: ~ # the strategy to adopt if some labels are missing
  task_labels_col: 'label'  # the column (for CSV files) of field (for JSON files) for the labels
  task_inputs_col: 'token' # the column (for CSV files) of field (for JSON files) for the input text
  multilabel_classification: True
  other_args: 
    depth: 3 # 1, 2 or 3 for wikivitals 
  debug_mode: False # activates debug mode if True (selection of a subset of the dataset for debugging purpose)
  debug_split_sizes: 1000 # used only if debug_mode is True, size of the subsets of train, validation, and test sets

model_args:
  # Model arguments
  name_or_path: './checkpoint-17750' # bert model to use (one can also specify a path)  ex: 'bert-base-uncased'
  classification_head: 'base_sequence_classifier' # value among 'base_sequence_classifier', 'with_word_attention'
  hidden_dropout_prob: 0.2 # dropout probability for the classification head
  freeze_bert: True # wether to freeze bert or not during training
  
preliminary_training_args:
  do_head_only_training: False
  training_args:
    num_train_epochs: 5
    load_best_model_at_end: False
    evaluation_strategy: 'no'

training_args:
  # Trainer arguments
  do_train: False
  do_eval: False
  do_predict: False
  evaluation_strategy: 'no' 
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 32
  optim: 'adamw_torch'
  gradient_accumulation_steps: 1
  eval_accumulation_steps: 1
  learning_rate: 1.e-4 # don't forget the '.'
  weight_decay: 0.1
  num_train_epochs: 5
  lr_scheduler_type: constant
  warmup_steps: 500
  logging_steps: 50
  seed: 42
  label_names: ~ # defaults to 'labels'
  report_to: 'tensorboard'
  save_total_limit: 1 # limits the total amount of checkpoints. Deletes the older checkpoints in output_dir.
  load_best_model_at_end: True
  metric_for_best_model: 'eval_loss'
  save_strategy: 'steps' # in ['no', 'steps', 'epoch'], don't forget the quotes if 'no'
  evaluation_strategy: 'steps'
  save_steps: 50
  eval_steps: 50
  output_dir: ./outputs_downstream_tasks # TO REMOVE
